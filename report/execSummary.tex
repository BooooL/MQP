\phantomsection
\addcontentsline{toc}{section}{Executive Summary}
\section*{Executive Summary}

Robotic solutions for remotely observing inaccessible areas through video streaming and localization are becoming a quickly expanding field. Currently, many of said solutions rely on a simple sensor suite consisting of cameras that transmit image data wirelessly for remote viewing by first responders or military personnel. Although these devices are highly useful, there is an opportunity to gain much more information from a similar sensor platform at the expense of more processing power. This tradeoff is usually taken at the side of the user, and a relatively powerful Personal Computer (PC) must be used to extract additional information from sensor data. 
\par
This project seeks to use similar sensors to those in existing solutions and couple them with the processing power of a Field Programmable Gate Array (FPGA) and System on Chip (SoC). This project will leverage FPGA technology in order to solve the current tradeoffs associated with remote mapping and observation through the creation of a proof of concept sensor suite. FPGAs are a clear candidate for such an implementation, since they are capable of performing the types of high-overhead calculations used in remote mapping through low-latency hardware parallelization. FPGAs also consume several orders of magnitude less power than standard computer processors, are highly cost efficient, and are a realistic solution for remote and battery operated devices. The result of this implementation is a proof of concept Simultaneous Localization and Mapping (SLAM) sensor suite that could serve as a replacement to a simple imaging sensor on a remote robotic platform. 
\par
The sensors used in this project include a pair of MT9V034 camera modules mounted on a customized stereo camera printed circuit board. The MT9V034 camera module is a global-shutter monochrome image sensor capable of capturing WVGA imagery at 60 frames per second. A Hokuyo URG-04LX scanning laser rangefinder is also used, and consists of a scanning infrared laser rangefinder with a 240$^\circ$ field of view and 99\% distance precision. Using an image processing technique known as disparity mapping, it is possible to convert stereo camera imagery into 3D depth maps that may then be correlated with laser rangefinder data. In order to geographically reference data, a Digilent PmodNAV Inertial Measurement Unit (IMU) is also used. The FPGA platform used is an Avnet ZedBoard, which contains a Xilinx Zynq7020 All-Programmable SoC. The Zynq family of Xilinx devices consist of a dual-core ARM Cortex A9 processor coupled with Xilinx Artix-7 FPGA fabric.
\par
On the input side of the system, the two stereo cameras are physically connect to two video memory buffers, and the camera controls and video memory buffer data are accessed using the Zynq processor's FPGA fabric. This interface has been implemented using a customized printed circuit board that was created during initial project development. Both the scanning laser rangefinder and IMU are connected directly to the dual-core ARM Cortex A9 processor of the Zynq IC, and are communicated with by using low-level peripheral controls. 
\par
The first stage of data processing implemented consists of a dual image buffer controller used for triggering image captures and for reading stereo camera imagery into local memory on the Zynq7020 processor. A 3D depth estimation algorithm then reads in image data from local memory, and calculates the relative offset between objects contained in the stereo image pair. A portion of this data is then stored in local memory for correlation with data from the 2D scanning laser rangefinder. Data from the rangefinder and IMU is simultaneously pre-processed on the ARM Cortex A9 processor, and then passed to the system's programmable logic to undergo further coordinate-axis transformation. Several user output modes have been included in the proof of concept implementation, including a raw image stream, 3D depth map, and combined 2D floorplan map.  
\par
This project successfully implements stereo cameras, a scanning laser rangefinder, and an IMU in order to create a real-time 3D depth map in addition to a compass-referenced 2D map of the objects closest to the device. Originally, this project proposal included the addition of human object detection and wireless data transmission for the creation of an all-encompassing sensor suite. However, many difficulties were faced throughout this project, and issues with developing customized camera hardware and rangefinder communications created the need for a revised overall goal that could be met within a single-semester project deadline.
\par
For future work, we recommend incorporating IMU displacement data so that the device can create a more realistic floorplan, compared to our device's radar-esque functionality. For this sensor suite to be utilized as intended, there must also be wireless data transmission for remote user access. In addition, incorporating human detection would be a luxury that could be added to the image processing portion of this project. A human detection algorithm could be implemented and combined with the existing 2D and 3D depth information in order to create an all-encompassing sensor suite. These proposed modifications were quickly found to be out of the scope of this project after beginning development, but would greatly improve this project's relevance and utility as a replacement for existing first responder remote observation products.
