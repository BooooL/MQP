\phantomsection
\addcontentsline{toc}{section}{Executive Summary}
\section*{Executive Summary}
Robotic solutions for remotely observing inaccessible areas through video streaming and localization are becoming a quickly expanding field. Currently, many of said solutions rely on a simple sensor suite consisting of cameras that transmit raw image data wirelessly for remote viewing by first responders or military personnel. Although these devices are highly useful, there is an opportunity to gain much more information from the same type of system through Simultaneous Localization and Mapping (SLAM). Through performing onboard image processing on the sensor suite itself, valuable information can be extracted from the images, such as depth information or even human detection, before its wireless transmission.
\par
This project sought to fill this gap in existing technology by creating a sensor suite that processed real-time stereo camera imagery onboard and combined it with localization and distance data to generate a 3D depth map and 2D floorplan of the sensor suite's environment. Field Programmable Gate Arrays (FPGAs) were a clear candidate for such an implementation since they are capable of performing the types of high-overhead calculations used in remote mapping through low-latency hardware parallelization. FPGAs also consume several orders of magnitude less power than standard computer processors, are highly cost efficient, and are a realistic solution for remote and battery operated devices. The FPGA platform used to create this proof of concept sensor suite was an Avnet ZedBoard, which contained a Xilinx Zynq-7000 All-Programmable SoC. The Xilinx Zynq-7000 SoC consists of a dual-core ARM Cortex A9 processor coupled with Xilinx Artix-7 FPGA fabric.
\par
The sensors used in this project included a pair of MT9V034 camera modules mounted on a customized stereo camera printed circuit board. The MT9V034 camera module is a global-shutter monochrome image sensor capable of capturing WVGA imagery at 60 frames per second. A Hokuyo URG-04LX scanning laser rangefinder was also used to estimate the distance of objects closest to the device. This rangefinder has a 240$^\circ$ field of view and 99\% distance precision. Using an image processing technique known as disparity mapping, the stereo camera imagery was converted into 3D depth maps that were correlated with rangefinder data. In order to geographically reference data with digital compass readings, a Digilent PmodNAV Inertial Measurement Unit (IMU) was also used. 
\par
The first stage of data processing consisted of a dual image buffer controller used for triggering image captures and reading stereo camera imagery into local memory on the Zynq-7000 processor. A 3D depth estimation algorithm then read in image data and calculated the relative offset between objects contained in the stereo image pair. A portion of this data was then stored in local memory for correlation with data from the 2D scanning laser rangefinder. Distance data from the rangefinder and compass data from the magnetometer were simultaneously pre-processed on the ARM Cortex A9 processor, and then passed to the system's programmable logic to undergo further coordinate-axis transformation. 
\par
Through its data processing stages, the sensor suite produced a real-time 3D depth map in addition to a compass-referenced 2D map of the objects closest to the device. These continuously updated outputs were viewable through the use of an attached VGA display, and were selectable based on user inputs. The overall result of this implementation was a proof of concept SLAM sensor suite that could serve as a replacement to a simple imaging sensor on a remote robotic platform.
\par
For future work, we recommend incorporating IMU displacement data in order to create visualizations of the entire path traversed by the device. In addition, incorporating human detection would be an improvement that could be added to the image processing portion of this project. A human detection algorithm could be combined with the existing 2D and 3D depth information in order to create an all-encompassing sensor suite for situational awareness analysis. 





