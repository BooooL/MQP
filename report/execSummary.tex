\phantomsection
\addcontentsline{toc}{section}{Executive Summary}
\section*{Executive Summary}
Robotic solutions for remotely observing inaccessible areas through video streaming and localization are becoming a quickly expanding field. Currently, many of said solutions rely on a simple sensor suite consisting of cameras that transmit raw image data wirelessly for remote viewing by first responders or military personnel. Although these devices are highly useful, there is an opportunity to gain much more information from the same type of system through Simultaneous Localization and Mapping (SLAM). Through performing onboard image processing on the sensor suite itself, valuable information can be extracted from the images, such as depth information or even human detection, before its wireless transmission.
\par
This project sought to fill this gap in existing technology by creating a sensor suite that processed real-time stereo camera imagery onboard and combined it with localization and distance data to generate a 3D depth map and 2D floorplan of the sensor suite's environment. Field Programmable Gate Arrays (FPGAs) were a clear candidate for such an implementation since they are capable of performing the types of high-overhead calculations used in remote mapping through low-latency hardware parallelization. FPGAs also consume several orders of magnitude less power than standard computer processors, are highly cost efficient, and are a realistic solution for remote and battery operated devices. The FPGA platform used to create this proof of concept sensor suite was an Avnet ZedBoard, which contained a Xilinx Zynq-7020 All-Programmable System on Chip (SoC). The Xilinx Zynq-7020 SoC consists of a dual-core ARM Cortex A9 processor coupled with Xilinx Artix-7 FPGA fabric. Having both an embedded processor and digital logic on the same chip allowed the sensor suite to be designed in a way that was able to use each for their unique advantages. The embedded processor was mainly used to communicate with the necessary peripherals using Xilinx's built-in peripheral drivers, whereas the digital logic was used to parallelize the data processing and interface with memory.
\par
The sensors used in this project included a pair of MT9V034 monochrome camera modules. These cameras were mounted on a custom-made stereo camera printed circuit board developed during the initial testing stages of the project. The MT9V034 camera module is a global-shutter image sensor capable of capturing WVGA imagery at 60 frames per second. A Hokuyo URG-04LX scanning laser rangefinder was also used to estimate the distance of objects closest to the device. This rangefinder has a 240$^\circ$ field of view and provides 99\% distance precision. In order to geographically reference data with digital compass readings, the magnetometer on a Digilent PmodNAV Inertial Measurement Unit (IMU) was also used. 
\par
The first stage of data processing consisted of a dual image buffer controller used for triggering image captures and reading stereo camera imagery into local memory on the Zynq-7020 processor. Using an image processing technique known as disparity mapping, the stereo camera imagery was converted into 3D depth maps. A portion of this data was then stored in local memory for correlation with data from the 2D scanning laser rangefinder. Distance data from the rangefinder and compass data from the magnetometer were simultaneously pre-processed on the ARM Cortex A9 processor, and then written to the system's programmable logic to undergo further coordinate-axis transformation. 
\par
Through its data processing stages, the sensor suite produced a real-time 3D depth map in addition to a compass-referenced 2D map of the objects closest to the device. These continuously updated outputs were viewable through the use of an attached VGA display, and were selectable based on user inputs. The overall result of this implementation was a proof of concept SLAM sensor suite that could serve as an improvement to a simple imaging sensor on a remote robotic platform.
\par
Existing robotic surveillance systems use camera modules to transmit real-time video streams of their surroundings. In addition to outputting a simple video stream, the proof of concept sensor suite developed through this project extracted depth information from stereo cameras and used it to create a real-time depth map. Simultaneously, the sensor suite created a compass-referenced 2D floorplan of its environment. The final product sensor suite proved that more situational awareness was available than was being provided by existing solutions, and that FPGAs were a viable tool for this type of application. 
\par
For future work, we recommend incorporating IMU displacement data in order to create visualizations of the entire path traversed by the device. In addition, incorporating human detection would be an improvement that could be added to the image processing portion of this project. A human detection algorithm could be combined with the existing 2D and 3D depth information in order to create an all-encompassing sensor suite for situational awareness analysis. 





