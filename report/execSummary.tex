\phantomsection
\addcontentsline{toc}{section}{Executive Summary}
\section*{Executive Summary}

Robotic solutions for remotely observing inaccessible areas through video streaming and localization are becoming a quickly expanding field. Currently, many of said solutions rely on a simple sensor suite consisting of cameras that transmit raw image data wirelessly for remote viewing by first responders or military personnel. Although these devices are highly useful, there is an opportunity to gain much more information from the same type of system through Simultaneous Localization and Mapping (SLAM). Through performing onboard image processing on the sensor suite itself, valuable information can be extracted from the images, such as depth information or even human detection, before their wireless transmission. Processing the data before it is wirelessly transmitted implies less processing on the receiving end, allowing the data to be accessed by less powerful devices.
\par
This project sought to fill this gap in existing technology by creating a sensor suite that processes real-time stereo camera imagery onboard and combines it with localization and distance data to generate a 3D depth map and 2D floorplan of the sensor suite's environment. Field Programmable Gate Arrays (FPGAs) were a clear candidate for such an implementation since they are capable of performing the types of high-overhead calculations used in remote mapping through low-latency hardware parallelization. FPGAs also consume several orders of magnitude less power than standard computer processors, are highly cost efficient, and are a realistic solution for remote and battery operated devices. The FPGA platform used to create this proof of concept sensor suite was an Avnet ZedBoard, which contains a Xilinx Zynq7020 All-Programmable SoC. The Xilinx Zynq7020 SoC consists of a dual-core ARM Cortex A9 processor coupled with Xilinx Artix-7 FPGA fabric.
\par
The sensors used in this project included a pair of MT9V034 camera modules mounted on a customized stereo camera printed circuit board. The MT9V034 camera module is a global-shutter monochrome image sensor capable of capturing WVGA imagery at 60 frames per second. A Hokuyo URG-04LX scanning laser rangefinder was also used to estimate the distance of objects closest to it. This rangefinder has a 240$^\circ$ field of view and 99\% distance precision. Using an image processing technique known as disparity mapping, the stereo camera imagery was converted into 3D depth maps that could then be correlated with rangefinder data. In order to geographically reference data, a Digilent PmodNAV Inertial Measurement Unit (IMU) was also used. 
\par
The first stage of data processing implemented consisted of a dual image buffer controller used for triggering image captures and for reading stereo camera imagery into local memory on the Zynq7020 processor. A 3D depth estimation algorithm then read in image data from local memory, and calculated the relative offset between objects contained in the stereo image pair. A portion of this data was then stored in local memory for correlation with data from the 2D scanning laser rangefinder. Distance data from the rangefinder and compass data from the IMU was simultaneously pre-processed on the ARM Cortex A9 processor, and then passed to the system's programmable logic to undergo further coordinate-axis transformation. 
\par
Through its data processing stages, the sensor suite produced a real-time 3D depth map in addition to a compass-referenced 2D map of the objects closest to the device. These outputs were viewable through the use of an attached VGA display, and were selectable based on user inputs. Both the 3D and 2D maps generated by the system were refreshed between frames, allowing only the most recent data to be displayed. The overall result of this implementation was a proof of concept SLAM sensor suite that could serve as a replacement to a simple imaging sensor on a remote robotic platform.
\par
For future work, we recommend incorporating IMU displacement data so that the device can create a more realistic floorplan containing more than a single rangefinder sweep of data. For this sensor suite to be utilized as intended, there must also be wireless data transmission for remote user access. In addition, incorporating human detection would be an improvement that could be added to the image processing portion of this project. A human detection algorithm could be implemented and combined with the existing 2D and 3D depth information in order to create an all-encompassing sensor suite. These proposed modifications would greatly improve this project's relevance and utility as a replacement for existing first responder remote observation products.





