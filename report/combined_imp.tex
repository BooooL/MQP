\subsection{Combined Implementation}
The following sections describe the integration of each individual sensor's data into the final, combined implementation. The rangefinder and disparity data was combined to produce accurate depth estimations. This data was also integrated with navigational data to create a more accurate approximation of the sensor suite's relative surroundings. 

\subsubsection{Rangefinder and Disparity Data Integration}
3D depth information from the disparity algorithm was combined with 2D depth information from the scanning laser rangefinder in order to increase the overall accuracy of the system. Since the rangefinder and stereo camera interface shared a horizontal viewing plane and both sensors gathered information on the same scene, there was some distinguishable overlap in sensor data. This overlap was taken advantage of in order to produce a more accurate 2D "floorplan" of the area being observed. This type of data integration is especially useful in situations where the scanning laser rangefinder is out of range.
\par
Through the use of a moving average Finite Impulse Response (FIR) filter across a horizontal line of depth information from the disparity algorithm output, a single line of depth information obtained was correlated with rangefinder data obtained from the same scene. Note that although 2D rangefinder data is organized using a polar coordinate scheme, the output buffer used for displaying rangefinder data via VGA contains the same data in a Cartesian format. This Cartesian rangefinder data was easily combined with averaged disparity depth information at the output stage, where both sensors' data is displayed relative to the same central location on screen.
\par
In order to correlate both sensors' data for a combined output mode, the field of view of each device was taken into account. Since each camera has an approximate 55$^\circ$ field of view, and camera imagery is 752 pixels wide, the stereo camera interface has a deg:pixel ratio of $\frac{752}{55}=13.67\frac{px}{deg}$. Output data from the rangefinder is divided into 768 steps over a 270$^\circ$ field of view. In order to correlate disparity data with rangefinder data, the averaged disparity depth line was converted an equivalent number of "steps" worth of data. The conversion factor for pixels of disparity depth to "steps" was calculated as shown by Equation \ref{yeeboi}.
\begin{equation} \label{yeeboi}
13.67\frac{px}{deg}*\frac{270^\circ}{768\,\,steps} = 4.8\frac{px}{step}
\end{equation}
\par
The output from the disparity pixel line was scaled down by an approximate factor of 4.8 in order for it to correlate with depth information from the scanning laser rangefinder. Once this scaling process was complete, depth information from the disparity algorithm was directly overlaid on the 2D scanning laser rangefinder's output in order to produce a combined depth map. 

\subsubsection{Accounting for Compass Data}
With compass data from the IMU's magnetometer, orientation was calculated. The IMU's compass data offset the rangefinder's step count in the programmable software. Changing the step count changes the start point location of the data processing, thus rotating the distance data around the device. This changes the direction of the rangefinder's $240^\circ$ "field of vision". For example, an IMU reading corresponding to due West is a rotation of $90^\circ$ counterclockwise from due North. By Equation \ref{StepsPerDegree}, $90^\circ$ equates to 256 rangefinder steps. So, the rangefinder's data essentially begins at step 256 and end at step 1024 or 0\footnote{step 768 + 256 step offset = 1024. Since there are 1024 rangefinder steps in a circle, step 1024 is the same as step 0.}, seen in Figure \ref{rangefinder_fov}.

\begin{equation}
	90^\circ \div \dfrac{360^\circ}{1024 \textrm{ } steps}  = 256 \textrm{ } steps
	\label{StepsPerDegree}
\end{equation}

\par
Due to time constraints, the IMU displacement data was not able to be incorporated into this project. The displacement data was planned to change the device's location on the VGA screen. Ideally, the device would appear to traverse the screen as the device itself traverses a location, with the rangefinder's data traveling with it. Since the rangefinder's data is stored in such a way that it is localized to the device's location, incorporating the IMU's displacement data is as easy as obtaining the data from the IMU in the PS and sending it to the PL via an extra PL read register discussed in Section \ref{sssec:creatingCustomIP}.


