\subsection{Combined Implementation}
The following sections describe the integration of each individual sensor's data into the final, combined implementation. 
%The rangefinder and disparity data was combined to produce accurate depth estimations. This data was also integrated with navigational data to create a more accurate approximation of the sensor suite's relative surroundings. 

\subsubsection{Rangefinder and Disparity Data Integration}
3D depth information from the disparity algorithm was combined with 2D depth information from the scanning laser rangefinder in order to increase the overall accuracy of the system. Since the rangefinder and stereo camera interface shared a horizontal viewing plane and both sensors gathered information on the same scene, there was some distinguishable overlap in sensor data. This overlap was taken advantage of in order to produce a more accurate 2D "floorplan" of the area being observed.
\par
This combined data stream relied on a moving average Finite Impulse Response (FIR) filter across a horizontal line of depth information from the disparity algorithm output. Note that although 2D rangefinder data was organized using a polar coordinate scheme, the output buffer used for displaying rangefinder data via VGA contained the same data in a Cartesian format. Cartesian rangefinder data was easily combined with averaged disparity depth information at the output stage, where both sensors' data was displayed relative to the same central location on screen.
\par
In order to correlate both sensors' data for a combined output mode, the field of view of each device needed to be taken into account. Since each camera had an approximate 55$^\circ$ field of view, and camera imagery was 752 pixels wide, the stereo camera interface had a deg:pixel ratio of $\frac{752}{55}=13.67\frac{px}{deg}$. In contrast, output data from the rangefinder was divided into 768 steps over a 270$^\circ$ field of view. In order to correlate disparity and rangefinder data, the averaged disparity depth line was converted an equivalent number of rangefinder "steps" worth of data. The conversion factor for pixels of disparity depth to "steps" was calculated as shown in Equation \ref{yeeboi}.
\begin{equation} \label{yeeboi}
13.67\frac{px}{deg}*\frac{270^\circ}{768\,\,steps} = 4.8\frac{px}{step}
\end{equation}
\par
The output from the disparity pixel line needed to be scaled down by an approximate factor of 4.8 in order for it to correlate with depth information from the scanning laser rangefinder. Once this scaling process was complete, depth information from the disparity algorithm was directly overlaid on the 2D scanning laser rangefinder's output in order to produce a combined depth map. 

\subsubsection{Accounting for Compass Data}
Device orientation was calculated using compass data from the IMU's magnetometer. This compass data was used to offset the rangefinder's step count in the programmable software. Changing the step count changed the start point location of the rangefinder sweep, thus rotating the distance data around the device. In turn, this changed the direction of the rangefinder's $240^\circ$ "field of vision". For example, a compass reading corresponding to due West was a rotation of $90^\circ$ counterclockwise from due North. By Equation \ref{StepsPerDegree}, $90^\circ$ equated to 256 rangefinder steps. So, the rangefinder's data essentially began at step 256 and ended at step 1024 or 0\footnote{step 768 + 256 step offset = 1024. Since there were 1024 rangefinder steps in a circle, step 1024 was the same as step 0.}, seen in Figure \ref{rangefinder_fov}.

\begin{equation}
	90^\circ \div \dfrac{360^\circ}{1024 \textrm{ } steps}  = 256 \textrm{ } steps
	\label{StepsPerDegree}
\end{equation}



