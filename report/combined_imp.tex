\subsection{Combined Implementation}
\subsubsection{Rangefinder and Disparity Data Integration}
In order to increase the overall accuracy of the system, 3D depth information from the disparity algorithm may be combined with 2D depth information from the scanning laser rangefinder. If the rangefinder and stereo camera interface share a horizontal viewing plane and both sensors are gathering information on the same scene, there will be some distinguishable overlap in sensor data. This overlap may be taken advantage of in order to produce a more accurate 2D "floorplan" of the area being observed. This type of data integration is especially useful in situations where the scanning laser rangefinder is out of range.
\par
Through the use of a moving average Finite Impulse Response (FIR) filter across a horizontal line of depth information from the disparity algorithm output, a single line of depth information may be obtained that can be correlated with rangefinder data obtained from the same scene. Note that although 2D rangefinder data is organized using a polar coordinate scheme, the output buffer used for displaying rangefinder data via VGA contains the same data in a Cartesian format. This Cartesian rangefinder data can easily be combined with averaged disparity depth information at the output stage, where both sensors' data is displayed relative to the same central location on screen. 
\par
In order to correlate both sensors' data for a combined output mode, the field of view of each device needs to be taken into account. Since each camera has an approximate 55$^\circ$ field of view, and camera imagery is 752 pixels wide, the stereo camera interface has a deg:pixel ratio of $\frac{752}{55}=13.67\frac{px}{deg}$. Output data from the rangefinder is divided into 768 steps over a 270$^\circ$ field of view. In order to correlate disparity data with rangefinder data, the averaged disparity depth line needs to be converted an equivalent number of "steps" worth of data. The conversion factor for pixels of disparity depth to "steps" may be calculated as shown by Equation \ref{yeeboi}.
\begin{equation} \label{yeeboi}
13.67\frac{px}{deg}*\frac{270^\circ}{768\,\,steps} = 4.8\frac{px}{step}
\end{equation}
\par
This means that the output from the disparity pixel line should be scaled down by an approximate factor of 4.8 in order for it to correlate with depth information from the scanning laser rangefinder. Once this scaling process is complete, depth information from the disparity algorithm may be directly overlaid on the 2D scanning laser rangefinder's output in order to produce a combined depth map. 

\subsubsection{Accounting for Navigational Data}
With the IMU's accelerometer, gyroscope, and magnetometer, displacement and orientation can be calculated. The orientation and displacement can be displayed on a screen to show the device's behavior and its location. This can be combined with the rangefinder's data to show the orientation of the device, where it has traversed, and the distance away of the objects closest to it.
\par
The IMU's compass data was combined with the rangefinder's step count. Changing the step count rotates the distance data around the device by changing the starting point of the data processing. This changes the direction of the rangefinder's $240^\circ$ field of vision. For example, an IMU reading corresponding to due West is a rotation of $90^\circ$ counterclockwise from due North. By Equation \ref{StepsPerDegree}, $90^\circ$ equates to 256 rangefinder steps. So, the rangefinder's data would essentially begin at step 256 and end at step 1024 or 0\footnote{step 768 + 256 step offset = 1024. Since there are 1024 rangefinder steps in a circle, step 1024 is the same as step 0.}, seen in Figure \ref{rangefinder_fov}.

\begin{equation}
	90^\circ \div \dfrac{360^\circ}{1024 \textrm{ } steps}  = 256 \textrm{ } steps
	\label{StepsPerDegree}
\end{equation}

\par
Due to time constraints, the IMU displacement data was not able to be incorporated into this project. The displacement data was planned to change the device's location on the VGA screen. Ideally, the device would appear to traverse the screen as the device itself traverses a location, with the rangefinder's data traveling with it. Since the rangefinder's data is stored in such a way that it is localized to the device's location, incorporating the IMU's displacement data should be as easy as obtaining the data from the IMU in the PS and sending it to the PL via an extra PL read register discussed in Section \ref{sssec:creatingCustomIP}.



