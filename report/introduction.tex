\section{Introduction}
In recent years, improvements in embedded processing technology have allowed for the creation of robotic situational awareness platforms for remotely observing dangerous or inaccessible areas. The market for such devices is a new and expanding field, and faces large demand from the military and first responders. This field is being piloted by throwable and remotely drivable platforms such as the Endeavor Robotics 110 FirstLook and the Bounce Imaging Explorer, shown in Figure \ref{robocop}.

\par
\begin{figure}[H]
       \begin{subfigure}[h]{0.5\textwidth}
            \centerline{\includegraphics[width=1.0\textwidth]{FirstLook.jpg}}
           \caption{Endeavor Robotics 110 FirstLook \cite{endeavor}}
       \end{subfigure}
       \begin{subfigure}[h]{0.5\textwidth}
           \centerline{\includegraphics[width=0.8\textwidth]{bounce_img.png}}
           \caption{Bounce Imaging Explorer \cite{bounceImaging}}
       \end{subfigure}
\caption{Robotic Situational Awareness Devices}
\label{robocop}
\end{figure}
\par

These devices contain simple wireless video streaming technology, allowing for remote visual surveillance and situational analysis. Although a video stream is an effective strategy for simple observation, this method of gathering information has room for improvement. This project investigated the extraction of information such as object positioning and localization from a camera-based sensor suite in real time, allowing for more comprehensive situational observation. One method of performing this process is Simultaneous Localization and Mapping, or SLAM.
\par
SLAM is the technique of mapping an unknown environment with respect to an agent, and can be performed using a wide variety of sensors and computational methods. SLAM is a common area of research in the fields of image processing and high-speed computing, and has been applied mainly to autonomous vehicles. Most current SLAM implementations rely on the use of a sensor suite connected to a computer or System on Chip (SoC) computing device.
\par
One type of technology useful for performing the high speed data processing necessary for SLAM is a Field Programmable Gate Array, or FPGA. FPGAs consist of digital logic gates that are designed to be user-configured, allowing for the creation of completely customized digital hardware. FPGAs are especially useful for parallelized data processing, posing potential real-time advantages over standard computing or microcontroller technology. Although FPGA technology is highly applicable to performing SLAM-like tasks, there are currently few existing products that use FPGAs for this purpose.
\par
This project explored the viability of an FPGA-based real-time SLAM sensor suite as a replacement for standard video cameras on existing situational awareness systems. This sensor suite utilized data from stereo camera modules, a scanning laser rangefinder, and an Inertial Measurement Unit (IMU) to create a real-time depth-augmented video feed and compass-referenced 2D floorplan of the device's field of view.
\par
The following chapters detail the creation of this device, beginning with an exploration of relevant technology and prior work. The overall system design and processing methods used are examined at a high level in the following section. Next, the implementation requirements of each individual sensor are explored in more detail. Methods for processing and combining sensor data for the production of a 2D floorplan and 3D depth visualization are then detailed. Each of these methods are then verified through comprehensive testing, and a finalized design is demonstrated. Lastly, conclusions and recommendations for future improvements are presented.




